{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3af7f893244b4285994fddc7b54e8128",
    "deepnote_cell_type": "markdown",
    "id": "6Og4DnJPrB4A"
   },
   "source": [
    "# Week 8 - Trees and Ensembles\n",
    "\n",
    "### Aims\n",
    "\n",
    "By the end of this notebook you will be able to understand: \n",
    "\n",
    ">* The Basics of Decision Trees\n",
    ">* Cost Complexity Pruning\n",
    ">* Ensembles (bagging, random forests, voting)\n",
    "\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Decision Trees](#intro)\n",
    "\n",
    "3. [Ensembles](#majVot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "04ba2861a4044d579f2eacdfc40358a1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "As usual, during workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- In some Exercises, you will see some hints at the bottom of questions.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "06143688836e4d33ad3a81390566f8e6",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "# Setup <a id='setup'></a>\n",
    "\n",
    "Let's load the packages needed for this workshop (more will be loaded through the workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4614984fddb14b63a147ca1d39a9c1b9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 59,
    "execution_start": 1709995759343,
    "id": "grVNp8GrrH0g",
    "output_cleared": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline  \n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# some new libraries and modules for this week\n",
    "from sklearn.tree import export_graphviz, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees <a id='intro'></a>\n",
    "\n",
    "Decision trees provide a simple, interpretable model for both classification and regression tasks. They are able to capture nonlinear relationships between the features and target variables, by learning simple decision rules and assuming piecewise constant approximations. The deeper the tree, the more complex the decision rules and the better the fit on the training data.\n",
    "\n",
    "Decision trees require little preprocessing and feature engineering. While other methods often require standardization, this is not the case for trees, and some tree and algorithm combinations can even support missing values. Decision trees able to handle both numerical and categorical data. However, sklearn does not implent more advanced methods for multi-class categorical features, and one-hot encoding must be used; if the number of features is large, this may result in many features and complex trees that suffer from overfitting. \n",
    "\n",
    "To learn the decision tree, sklearn uses an optimized version of the Classification and Regression Tree (CART) algorithm, which is a greedy algorithm where locally optimal decisions are made at each node, starting from the root down to the leaves. At each node/step, CART uses binary spliting and finds the threshold yielding the largest information gain. For more information on sklearn's implementation of decision trees, see https://scikit-learn.org/stable/modules/tree.html#tree-classification.\n",
    "\n",
    "The sklearn implementation supports both regression tasks through [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) and classication through [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). In this workshop, we will focus on **classification**. \n",
    "\n",
    "Let's start by loading data to explore the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "030a8ba0e7ff4b499b870dce49189796",
    "deepnote_cell_type": "markdown",
    "id": "N8_vjOPKdqLm"
   },
   "source": [
    "##  Palmer Penguin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7c5894d8a3154f678280c530ee42f9e8",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "The palmer penguins dataset contains information for 344 penguins from 3 different species and from 3 islands in the Palmer Archipelago, Antarctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6112032875dc49fdb15851111631d1a2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 153,
    "execution_start": 1709995778584,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some basic EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a89fd5f54e2840e2b843d3aeab281b9c",
    "deepnote_cell_type": "code",
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true
   },
   "source": [
    "Notice that there are some missing values. For simplicity, we will remove them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7c065953013445f38be5d374384900a7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 174,
    "execution_start": 1709995782053,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# dropna values\n",
    "penguins_rm = penguins.dropna()\n",
    "penguins_rm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the task of classifying species. Notice that there are four numerical features and two categorical features. For simplicity, we will focus on the numerical features only. Before any further EDA, we split our data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "# Feature matrix and response vector\n",
    "X, y = penguins_rm.drop([\"island\", \"sex\", \"species\"], axis=1), penguins_rm['species']\n",
    "\n",
    "# Stratify split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle= True, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "625c530465f84d8d83d5fba0ec18e7cf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "- Draw a pairplot, setting `hue = \"species\"`, to explore the numerical features relation with species. \n",
    "- Comment on any relationships that you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4125e65591c74804b3c3ac8c851b8f8f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4980,
    "execution_start": 1709995793228,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Pair plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "21b4bf31f6ab4efdada1e8f7ffd3f979",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "\n",
    "Recall from lectures, the decision tree has a number of parameters that affect the shape of the tree:\n",
    "\n",
    "- `max_depth` : the maximum depth of the tree. \n",
    "\n",
    "- `min_samples_split` : the minimum number of samples required to split an internal node\n",
    "\n",
    "- `min_samples_leaf`: the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
    "\n",
    "- `max_leaf_nodes`: the maximum number of leaf nodes. \n",
    "\n",
    "- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "As usual, once we create our `DecisionTreeClassifier` object, we can fit the tree by calling `.fit()`. Once fit, the tree will have a number of attributes and we can call additional methods such as `.predict()` or  `.predict_proba()` to predict hard or soft labels, respectively, for any new data. \n",
    "\n",
    "Let's fit our first decision tree. In the following code, we fit a simple tree of depth 2 by specifying `max_depth=2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install graphviz\n",
    "#! pip install graphviz\n",
    "import graphviz\n",
    "\n",
    "# Decsion tree with max depth of 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "tree_data = export_graphviz(\n",
    " tree_clf,\n",
    " rounded = True, filled = True, feature_names=X_train.columns, class_names = tree_clf.classes_\n",
    " )\n",
    "graphviz.Source(tree_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If you are using noteable, [inkscape](https://inkscape.org/release/inkscape-1.4/) is not installed, which is required in order to export the notebook to PDF, when graphviz images are included. In this case, please instead you the `plot_tree` function below to plot the tree instead (delete the code using graphviz). If you are running locally, please install graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import plot_tree\n",
    "\n",
    "#plot_tree(tree_clf, rounded = True, filled = True, feature_names=X_train.columns, class_names = tree_clf.classes_)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4ca53f534b1b40699dce12247a2454e5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 2  (CORE)\n",
    "\n",
    "Below, we print out the first five rows of the test data. Based on the fitted tree above (not using `.predict()`!!), what is the classified species of each penguin/row? what path along the decision tree is taken to classify the penguin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first five rows\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "07ffee1bc3464e9ea7d5fe15a0e7d925",
    "deepnote_cell_type": "markdown",
    "id": "4xhjuXdcvZ7v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 3  (CORE) \n",
    "\n",
    "Fit a new decision tree, again with a depth 2, but change the criterion to measure the quality of a split to entropy. How has the tree stucture changed?\n",
    "\n",
    "**Hint:** when creating the decision tree object, set the option `criterion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0af370321f564c0aa37383500b2e6b40",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 4  (CORE) \n",
    "\n",
    "- Create two functions, one to compute the Gini index and another to compute the entropy, based on a vector of counts representing the number of observations in each category. \n",
    "\n",
    "- For each leaf node in the trees fitted above, use your functions to compute the Gini index and entropy based on the counts reported. If your functions are correct, it should match the numbers reported in the tree.\n",
    "\n",
    "**RECALL**\n",
    "\n",
    "- **Gini Index**: is a measure of total variance across the $C$ classes, defined as:\n",
    "\n",
    "$G(R_m) = \\sum_{c=1}^{C} \\widehat{p}_{m,c} (1 - \\widehat{p}_{m,c}) = \n",
    "1 - \\sum_{c=1}^{C} \\widehat{p}_{m,c}^2$ \n",
    "\n",
    "where $\\widehat{p}_{m,c}$ is the proportion of training observations in the $m$ th region (belonging to $m$ th node) coming from the $c$ th class.\n",
    "\n",
    "- **Entropy**: is an information theoretic measure defined as:\n",
    "\n",
    "$H(R_m) = - \\sum_{c=1}^{C} \\widehat{p}_{m,c} \\log_2 \\widehat{p}_{m,c}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8a2f7f46a5034e52bd6f32744e1ad925",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 83,
    "execution_start": 1709996137662,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the gini index\n",
    "\n",
    "\n",
    "# Define a function to compute the gini index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gini for each leaf node\n",
    "\n",
    "\n",
    "# Compute the entropy for each leaf node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our fitted `DecisionTreeClassifier` stores the tree as an attribute, which can be accessed by calling `.tree_`. For more information on how `sklearn` stores a tree, see [here](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py). The fitted tree has a number of attributes, including `.impurity` providing the impurity for all nodes.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2b7f6c2edf204ddc8dc77aa207ce0d0f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1709996140452,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# The impurity values for all nodes are accessible in the impurity attribute of the tree.\n",
    "print(tree_clf.tree_.impurity)\n",
    "print(tree_clf2.tree_.impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4edf524f1f8c4963b1964fbdb040f5a1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "Trying increasing the maximum depth to 10.\n",
    "\n",
    "- Fit and visualize the tree.\n",
    "\n",
    "- What is the depth of the tree? **Hint**: check the available methods for a `DecisionTreeClassifier`. \n",
    "\n",
    "- How does the accuracy on both the train and test data compare with our previous tree with a depth of 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Complexity Pruning\n",
    "\n",
    "Instead of using cross-validation to fit and tune the tree depth and shape across a range of values, a more effective and computationally efficient strategy is **cost complexity pruning**. Recall that in cost complexity pruning, we start by constructing a complex tree, and sequentially prune back the tree by snipping off the branches that minimize the cost complexity. This produces a sequence of nested trees, with each one associated to range of values of $\\alpha$ (i.e. each tree is the optimal, unique tree for this range of $\\alpha$).\n",
    "\n",
    "We can perform cost complexity pruning in `sklearn` by using the method `cost_complexity_pruning_path()` on our decision tree object. Recall that a larger value of $\\alpha$ implies more regularization and more shallow trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost complexity pruning\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "path = tree_clf.cost_complexity_pruning_path(X_train,y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(ccp_alphas, impurities, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs alpha on the training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to understand the depth and number of nodes in each tree in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and store the decision tree for each value of alpha\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "\n",
    "# Plot the depth and number of nodes for each tree for in the sequence as a function of alpha\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to compute the accuracy of each tree in the sequence on both the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy is of course a decreasing function of $\\alpha$, as the tree becomes less complex as the $\\alpha$ increases. Instead, the test accuracy is fairly flat for any $\\alpha<0.2$. Of course, we can use the test data to choose $\\alpha$, as that would lead to **data leakage**!! Instead, we need to use cross-validation to tune the value of $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (CORE)\n",
    "\n",
    "Run the following code to compute the cross-validated accuracy across the values of $\\alpha$. \n",
    "\n",
    "- Add the cross-validated accuracy to the figure above with train and test accuracy.\n",
    "\n",
    "- What is best value of $\\alpha$ returned by the grid search? Based on the plot, would you choose this value or a different one? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "clf = DecisionTreeClassifier()\n",
    "gs = GridSearchCV(clf, \n",
    "                  param_grid = {'ccp_alpha': ccp_alphas}, \n",
    "                  cv = KFold(5, shuffle=True, random_state=42),)\n",
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, gs.cv_results_['mean_test_score'], marker=\"o\", label=\"cross-validation\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "For your chosen value of $\\alpha$, fit and visualize the tree. How does it compare to the shallow and deep tree that we initialy fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: call your tree clf or change to your name of the pruned tree when computing feature importances before exercise 12\n",
    "\n",
    "# Fit the tree\n",
    "\n",
    "# Visualize the tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "30e7b2a252aa4f75a0f817a6100728e6",
    "deepnote_cell_type": "markdown",
    "id": "_YZt5su8C7WZ"
   },
   "source": [
    "# Ensembles <a id='majVot'></a> \n",
    "\n",
    "Ensembles combine the predictions of several base estimators to improve generalization and robustness, compared with a single estimator. A number of ensemble methods are available in `sklearn`, and for an overview, see \n",
    "https://scikit-learn.org/stable/modules/ensemble.html#. However, we only have time to cover a subset of ensemble methods, namely, **bagging**, **random forests**, and **voting**.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "A bagging classifier is an ensemble of base classifiers, each fit on random subsets of a dataset. Their predictions are then pooled or aggregated to form a final prediction. To apply bagging to decision trees, we simply construct \n",
    "(i) decision trees using bootstrapped training sets,\n",
    "(ii) agregated the predictions (either by averaging the probabilities or by majority vote).\n",
    "\n",
    "Bagging is typically used as a way to reduce the variance of a high-variance estimator (e.g. a decision tree). The parameter `n_estimators` contols the number of bootstrapped datasets, and hence the number of estimators in the ensemble. While bagging is when sampling is performed with replacement, the `sklearn` implementation allows for more general constructions of the random subsets of the data: \n",
    "\n",
    "- Pasting is when the sampling is done without replacement and is designed to use smaller sample sizes than the training dataset (useful when the training dataset does not fit into memory). Pasting can be performed with `BaggingClassifier` by setting the options `bootstrap=FALSE` and `max_samples` either to an integer or a float representing the sample size of the random subset as a proportion of training points. \n",
    "\n",
    "- Random Subspaces is when random subsets of the dataset are drawn as random subsets of the features. Random Subspaces can be performed with `BaggingClassifier` by setting the options `bootstrap=FALSE` and `max_features` either to an integer or a float representing the number of features in the random subset as a proportion of the number of features in the training data. \n",
    "\n",
    "- Random Patches is when random subsets of the dataset are drawn based on random subsets of both the features and samples, and can be constructed by combining the options above. \n",
    "\n",
    "Averaging methods generally work best when the classifiers are diverse, and they have been shown to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.\n",
    "\n",
    "Note that while the default estimator in `BaggingClassifier` is a decision tree, other classifiers can be combined by specifying the option `estimator`. Also note that in `sklearn`'s implementation, the final predictions are obtained by averaging the prediction probabilities across estimators, with majority voting only used for estimators that don't provide probabilistic outputs. \n",
    "\n",
    "For more details, see:  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b0791cc0cd4b407aa102bf1b2b3a1357",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "\n",
    "### ðŸš© Exercise 8  (CORE)\n",
    "\n",
    "- Fit a bagged classifier to the data that combines 10 decision trees.\n",
    "\n",
    "- Plot the confusion matrix on the both test and training data for both your pruned tree and bagged classifier. Comment on the advantages and disadvantages of the pruned tree vs bagged classifier briefly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4ab3d2fd33c446bba544b48b65bb4565",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1709996203604,
    "id": "i2qXwFXvQbBq",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ae5c11871acb42d5a97af1857038fa12",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3e03f5b03b314e60b103d118dfebbb27",
    "deepnote_cell_type": "markdown",
    "id": "ZecdeX9yQdVh"
   },
   "source": [
    "### ðŸš© Exercise 9  (CORE)\n",
    "\n",
    "For the range of `n_estimators` provided below: \n",
    "- Fit the bagged trees and compute the training accuracy, test accuracy, and out-of-bag scores. \n",
    "- Plot all three metrics as a function of the number of estimators. \n",
    "- Comment on the results and suggest a suitable number of estimators for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of number of estimators\n",
    "n_est = np.array([1,5,10,20,50],dtype=int)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging: low diversity in estimators\n",
    "\n",
    "Bagging alone may not produce very diverse trees, and thus may result in only modest improvements in accuracy.  In particular, if there is a strong set of predictors in the data, then the bagged trees will look quite similar to each other and predictions will be highly correlated. Averaging highly correlated quantities does not lead to a large reduction in variance and provides on very small improvements in predictions.\n",
    "\n",
    "Run the following code below, the print out the feature and threshold used for the first split for each of the trees in our ensemble. Notice how most of trees use the same first splitting rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first split for all trees in the bagged model\n",
    "for i in range(10):\n",
    "    print('Estimator: ', i+1, \n",
    "          ', First split feature: ', bc.estimators_[i].tree_.feature[0], \n",
    "          ', First split threshold: ', bc.estimators_[i].tree_.threshold[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f8ef40cd87e5454cbe72ea1629a44d92",
    "deepnote_cell_type": "markdown",
    "id": "TLaXXgVqGQrd"
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests is one of the most widely used machine learning algorithms. A random forest is an ensemble that fits a number of decision tree classifiers on various random subsets of the data (Random Patches) and uses averaging to improve the predictive accuracy and control overfitting. Random forests are essentally bagged tree classifiers, but improve the diversity and decorrelate the trees compared with bagging by using a random sample of features each time a split in a tree is considered. The random forest algorithm can therefore be summarized as:\n",
    "\n",
    "- Draw a random bootstrap sample of the training data.\n",
    "- Grow a decision tree from the bootstrap data. At each node:\n",
    "    * Randomly select a subset of features without replacement (defaults to the square root of the total number of features).\n",
    "    * Split the node using the feature and threshold that provides the best split according to the objective function.\n",
    "- Repeat the steps above for the desired number of bootstrapped datasets (estimators in the ensemble).\n",
    "- Aggregate the predictions (`sklearn`'s `RandomForestClassifier` aggregates the prediction probabilities).\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Important parameters include:\n",
    "\n",
    "- `max_features`: specifies that number of random features to consider at each split. If set to the total number of features, then it is simply bagging. \n",
    "- `n_estimators`: number of trees in the ensemble; larger is generally better as averaging more trees will yield a more robust ensemble. \n",
    "- `max_samples`: sample size of the bootstrap dataset (useful when the training set is large to reduce memory and computational cost).\n",
    "\n",
    " By default, large and complex trees are grown for each bootstrapped data. While these trees tend to have high variance and overfit to the training data, aggregation reduces variance and improves predictive accuracy. If desired, the complexity of the trees can also be limited by setting the parameters of the decision trees  (e.g. `max_depth`, `max_leaf_nodes`); again this may be useful to reduce memory and computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dcf91481589e4fb9ac9376f738d0e70f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 10 (CORE)\n",
    "\n",
    "- Fit a random forest classifer that includes 10 trees in the ensemble. \n",
    "- Print the splitting rule (feature and threshold) for each estimator. How does this compare to the bagged model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cebfe85b8d2641b6bc560ffabbdbfb14",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9bb0b88f00404a5da057b698beb7c661",
    "deepnote_cell_type": "markdown",
    "id": "z4NLHMUEdKxP"
   },
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "- Fit a random forest to your data, using the OOB error to tune the number of features considered at each split, for **max_features** in $[2, 3, 4]$. What is the optimal number of features?\n",
    "- Visualize the confusion matrix on the train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b160764e5fff4c6cbe3637f025acc345",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 34724,
    "execution_start": 1709996292934,
    "id": "9vTpIe932BFz",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Note: let rf be the name your random forest object with the optimal max_features or change the name in exercise 12\n",
    "\n",
    "# Fit the model for each value of max features and compute the oob score\n",
    "\n",
    "\n",
    "# Print the best model parameter \n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance <a id='feImport'></a> \n",
    "\n",
    "The fitted `RandomForestClassifier` also has an attribute `feature_importances_`, that reflects the relative rank (i.e. depth) and predictive power of a feature, averaged across all trees in the ensemble. For further details, see https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation, and for an example, see https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html.\n",
    "\n",
    "Let's start by visualizing the feature importances for our pruned tree in exercise 7 (note: replace `clf` with the name of your tree from exercise 7, if not already called `clf`). Notice how this reflects the depth of the feature and impurity decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: change clf to name of your pruned tree in exercise 7\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = clf.feature_names_in_ \n",
    "\n",
    "# Create a pandas series with the feature importances\n",
    "importances = pd.Series(\n",
    "    clf.feature_importances_, index=feature_names\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "# Plot the feature importances\n",
    "ax = importances.plot.barh()\n",
    "ax.set_title(\"Decision Tree: Feature Importances\")\n",
    "ax.figure.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 12 (CORE)\n",
    "\n",
    "- Run the following code to plot the feature importance (averaged across all trees along with the standard deviation). \n",
    "- Visualize the correlation matrix of the features. \n",
    "- Has the ranking of the features changed comparing the pruned tree to the random forest? Does the correlation matrix provide any insight on why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: change rf to the name your random forest object in exercise 12\n",
    "\n",
    "# Compute the standard deviation \n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# Create a pandas series with the feature importances\n",
    "importances_rf = pd.Series(\n",
    "    rf.feature_importances_, index=feature_names\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "# Plot the feature importances\n",
    "ax = importances_rf.plot.barh(xerr=std)\n",
    "ax.set_title(\"Random Forest: Feature Importances\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3e89ce1a03d47b48a2387d45797ecfe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Voting Classifier\n",
    "\n",
    "While bagging and random forests focused on creating an ensemble of the same estimator (e.g. trees) fit on different random subsets of the data, the idea behind **voting** is to combine different estimators. The predictions then can be aggregated either by using a majority vote or by averaging predicted probabilities (soft vote). This can be useful for a set of equally well performing models in order to balance out their individual weaknesses.\n",
    "\n",
    "In the `VotingClassifier`, the default is to aggregate predictions with **hard voting** (i.e. `voting='hard'` by default), which simply looks at the tally of votes received by each class across the estimators in the ensemble. Instead, **soft voting** (i.e. `voting='soft'`) averages the predicted probabilities across the estimators in the ensemble.\n",
    "\n",
    "For further details, please see the documentation from here: https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "20ca353e2e4f4bbe96a4cd321ae54504",
    "deepnote_cell_type": "markdown",
    "id": "wYXQZYkTMy7M"
   },
   "source": [
    "### ðŸš© Exercise 13  (EXTRA)\n",
    "\n",
    "- Create a voting classifier that combines your pruned tree, random forests, logistic regression, and support vector classifier.\n",
    "\n",
    "- How does the accuracy on the test data compare across each model individually and the voting classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b973b29d712144a88a58c2995fd6c2e2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 538,
    "execution_start": 1709996354380,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit a logisitic regression with no penalty\n",
    "\n",
    "\n",
    "# Fit a SVC\n",
    "\n",
    "\n",
    "# Fit a voting classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b9cdf939eb5145a88695171eca2cf8df",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9e3e5a88e00b4f0ca737919fddd56cd6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week08.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "MLPy Workshop 8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
